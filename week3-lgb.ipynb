{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid-19 Week 3 light gbm model\n",
    "\n",
    "week 3 of COVID-19 reseasrch using light gbm as base learner.\n",
    "\n",
    "Please upvote:) and enjoy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,roc_auc_score\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized label encoding that leaves NaN value as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyLabelEncodeSingle(col):\n",
    "    levels=col.unique().tolist()\n",
    "    for l in levels:\n",
    "        if l is np.nan:\n",
    "            levels.remove(np.nan)\n",
    "    levelmap={e:i for i,e in enumerate(levels)}\n",
    "    return col.map(levelmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External dataset of my choise, data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jasonbenner/world-bank-datasets#World_Happiness_Index.csv\n",
    "world_happiness = pd.read_csv(\"../input/world-bank-datasets/World_Happiness_Index.csv\")\n",
    "world_happiness=world_happiness.iloc[:,:19]\n",
    "world_happiness.columns=[c.replace('(','').replace(')','').replace('(','').replace(',','').replace('-','_').replace('/','_').replace(' ','_') \n",
    "                               for c in world_happiness.columns]\n",
    "average_year={}\n",
    "temp_matrix=world_happiness.iloc[:,2:]\n",
    "for y in world_happiness.Year.unique():\n",
    "    average_year[y]=temp_matrix.loc[world_happiness.Year==y,:].mean()\n",
    "del temp_matrix\n",
    "gc.collect()\n",
    "distance=0\n",
    "while world_happiness.isna().sum().sum()!=0:\n",
    "    for y in world_happiness.Year.unique():\n",
    "        yhat=y-distance\n",
    "        if yhat>2018:\n",
    "            yhat=2018\n",
    "        elif yhat<2005:\n",
    "            yhat=2005\n",
    "        for c in world_happiness.columns[2:]:\n",
    "            world_happiness.loc[world_happiness.Year==y,c]=world_happiness.loc[world_happiness.Year==y,c].fillna(average_year[yhat][c])\n",
    "        yhat=y+distance\n",
    "        if yhat>2018:\n",
    "            yhat=2018\n",
    "        elif yhat<2005:\n",
    "            yhat=2005\n",
    "        for c in world_happiness.columns[2:]:\n",
    "            world_happiness.loc[world_happiness.Year==y,c]=world_happiness.loc[world_happiness.Year==y,c].fillna(average_year[yhat][c])\n",
    "        distance += 1\n",
    "world_happiness_latest = world_happiness.groupby('Country_name').nth(-1)\n",
    "world_happiness_first = world_happiness.groupby('Country_name').agg('first')\n",
    "world_happiness_last = world_happiness.groupby('Country_name').agg('last')\n",
    "world_happiness_count = world_happiness.groupby('Country_name').count()\n",
    "world_happiness_range=(world_happiness_last-world_happiness_first)/world_happiness_count\n",
    "world_happiness_range.drop(\"Year\", axis=1, inplace=True)\n",
    "world_happiness_latest.drop(\"Year\", axis=1, inplace=True)\n",
    "world_happiness_range.columns=[c+'_range' for c in world_happiness_range.columns]\n",
    "world_happiness_latest.columns=[c+'_latest' for c in world_happiness_latest.columns]\n",
    "world_happiness_grouped=pd.concat((world_happiness_latest,world_happiness_range),axis=1).reset_index()\n",
    "world_happiness_grouped[\"geography\"] = world_happiness_grouped.Country_name.astype(str)\n",
    "world_happiness_grouped.drop(\"Country_name\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')\n",
    "#X_test = pd.merge(left=X_test, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')\n",
    "#X_train.drop(\"Country_name\", axis=1, inplace=True)\n",
    "#X_test.drop(\"Country_name\", axis=1, inplace=True)\n",
    "malaria_world_health = pd.read_csv(\"../input/world-bank-datasets/Malaria_World_Health_Organization.csv\")\n",
    "malaria_world_health.columns=[c.replace(' ','_') for c in malaria_world_health.columns]\n",
    "malaria_world_health[\"geography\"] = malaria_world_health.Country.astype(str)\n",
    "malaria_world_health.drop(\"Country\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\n",
    "#X_test = pd.merge(left=X_test, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\n",
    "#X_train.drop(\"Country\", axis=1, inplace=True)\n",
    "#X_test.drop(\"Country\", axis=1, inplace=True)\n",
    "human_development = pd.read_csv(\"../input/world-bank-datasets/Human_Development_Index.csv\")\n",
    "human_development.columns=[c.replace(')','').replace('(','').replace(' ','_') for c in human_development.columns]\n",
    "human_development['Gross_national_income_GNI_per_capita_2018']= human_development['Gross_national_income_GNI_per_capita_2018'].apply(lambda x: x if x!=x else x.replace(',','')).astype(float)\n",
    "human_development[\"geography\"] = human_development.Country.astype(str)\n",
    "human_development.drop(\"Country\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\n",
    "#X_test = pd.merge(left=X_test, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\n",
    "#X_train.drop(\"Country\", axis=1, inplace=True)\n",
    "#X_test.drop(\"Country\", axis=1, inplace=True)\n",
    "#https://www.kaggle.com/nightranger77/covid19-demographic-predictors\n",
    "night_ranger = pd.read_csv(\"../input/covid19-demographic-predictors/covid19_by_country.csv\")\n",
    "night_ranger.columns=[c.replace(' ','_') for c in night_ranger.columns]\n",
    "night_ranger = night_ranger[night_ranger.Country != \"Georgia\"]\n",
    "night_ranger=night_ranger[['Country','Median_Age','GDP_2018','Crime_Index','Population_2020','Smoking_2016','Females_2018']]\n",
    "night_ranger[\"geography\"] = night_ranger.Country.astype(str)\n",
    "night_ranger.drop(\"Country\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=night_ranger_predictors, how='left', left_on='Country_Region', right_on='Country')\n",
    "#X_test = pd.merge(left=X_test, right=night_ranger_predictors, how='left', left_on='Country_Region', right_on='Country')\n",
    "#X_train.drop(\"Country\", axis=1, inplace=True)\n",
    "#X_test.drop(\"Country\", axis=1, inplace=True)\n",
    "#https://www.kaggle.com/hbfree/covid19formattedweatherjan22march24\n",
    "weather_df = pd.read_csv(\"../input/covid19formattedweatherjan22march24/covid_dataset.csv\")\n",
    "weather_df=weather_df[['Province/State',\n",
    "'Country/Region',\n",
    "'lat',\n",
    "'long',\n",
    "'day',\n",
    "'pop',\n",
    "'urbanpop',\n",
    "'density',\n",
    "'medianage',\n",
    "'smokers',\n",
    "'health_exp_pc',\n",
    "'hospibed',\n",
    "'temperature',\n",
    "'humidity']]\n",
    "weather_df[\"geography\"] = weather_df['Country/Region'].astype(str) + \": \" + weather_df['Province/State'].astype(str)\n",
    "weather_df.loc[weather_df['Province/State'].isna(), \"geography\"] = weather_df[weather_df['Province/State'].isna()]['Country/Region']\n",
    "weather_df.drop(['Country/Region','Province/State'], axis=1, inplace=True)\n",
    "weather_df=weather_df.replace(-999,np.nan)\n",
    "#weather_df['Province/State']=weather_df['Province/State'].fillna('Unknown')\n",
    "weather_df['day']=pd.to_datetime('2020-01-22')+weather_df['day'].apply(lambda x: timedelta(days=x))\n",
    "weather_df['month']=weather_df['day'].dt.month\n",
    "weather_df.drop('day',axis=1,inplace=True)\n",
    "weather_df=weather_df.groupby(['geography','month']).mean().reset_index()\n",
    "weather_df_latest = weather_df.groupby(['geography']).nth(-1).reset_index()\n",
    "weather_df_latest['month']=4\n",
    "weather_df=pd.concat((weather_df,weather_df_latest),sort=True,axis=0,ignore_index=True)\n",
    "\n",
    "#X_train = pd.merge(left=X_train, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country/Region','Province/State','month'])\n",
    "#X_test = pd.merge(left=X_test, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country/Region','Province/State','month'])\n",
    "#X_train.drop(['Country/Region','Province/State'], axis=1, inplace=True)\n",
    "#X_test.drop(['Country/Region','Province/State'], axis=1, inplace=True)\n",
    "#https://www.kaggle.com/londeen/world-happiness-report-2020\n",
    "happiness_df = pd.read_csv(\"../input/world-happiness-report-2020/WHR20_DataForFigure2.1.csv\")\n",
    "happiness_df.columns=[c.replace(':','').replace('+','').replace(' ','_') for c in happiness_df.columns]\n",
    "happiness_df['Regional_indicator']=MyLabelEncodeSingle(happiness_df['Regional_indicator'])\n",
    "happiness_df[\"geography\"] = happiness_df.Country_name.astype(str)\n",
    "happiness_df.drop(\"Country_name\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\n",
    "#X_test = pd.merge(left=X_test, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\n",
    "#X_train.drop('Country_name', axis=1, inplace=True)\n",
    "#X_test.drop('Country_name', axis=1, inplace=True)\n",
    "#https://www.kaggle.com/alizahidraja/world-population-by-age-group-2020\n",
    "age_df = pd.read_csv(\"../input/world-population-by-age-group-2020/WorldPopulationByAge2020.csv\")\n",
    "age_df['AgeGrp']=MyLabelEncodeSingle(age_df['AgeGrp'])\n",
    "def processAge(df):\n",
    "    ageindex=df['AgeGrp']\n",
    "    sexsum=df[['PopMale', 'PopFemale', 'PopTotal']].sum()\n",
    "    mp=sexsum['PopMale']/sexsum['PopTotal']\n",
    "    fp=sexsum['PopFemale']/sexsum['PopTotal']\n",
    "    p0=df.loc[ageindex==0,'PopTotal'].values[0]/sexsum['PopTotal']\n",
    "    p1=df.loc[ageindex==1,'PopTotal'].values[0]/sexsum['PopTotal']\n",
    "    p2=df.loc[ageindex==2,'PopTotal'].values[0]/sexsum['PopTotal']\n",
    "    p3=df.loc[ageindex==3,'PopTotal'].values[0]/sexsum['PopTotal']\n",
    "    m0=df.loc[ageindex==0,'PopMale'].values[0]/sexsum['PopMale']\n",
    "    m1=df.loc[ageindex==1,'PopMale'].values[0]/sexsum['PopMale']\n",
    "    m2=df.loc[ageindex==2,'PopMale'].values[0]/sexsum['PopMale']\n",
    "    m3=df.loc[ageindex==3,'PopMale'].values[0]/sexsum['PopMale']\n",
    "    f0=df.loc[ageindex==0,'PopFemale'].values[0]/sexsum['PopFemale']\n",
    "    f1=df.loc[ageindex==1,'PopFemale'].values[0]/sexsum['PopFemale']\n",
    "    f2=df.loc[ageindex==2,'PopFemale'].values[0]/sexsum['PopFemale']\n",
    "    f3=df.loc[ageindex==3,'PopFemale'].values[0]/sexsum['PopFemale']\n",
    "    return pd.DataFrame({'MaleP':mp,'MaleP_0':m0,'MaleP_1':m1,'MaleP_2':m2,'MaleP_3':m3,'FemaleP':fp,\n",
    "                         'FemaleP_0':f0,'FemaleP_1':f1,'FemaleP_2':f2,'FemaleP_3':f3,'PopTotal':sexsum['PopTotal'],\n",
    "                         'Pop_0':p0,'Pop_1':p1,'Pop_2':p2,'Pop_3':p3},index=[0])\n",
    "age_df=age_df.groupby('Location').apply(processAge).reset_index().drop('level_1',axis=1)\n",
    "age_df[\"geography\"] = age_df.Location.astype(str)\n",
    "age_df.drop(\"Location\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=age_df, how='left', left_on='Country_Region', right_on='Location')\n",
    "#X_test = pd.merge(left=X_test, right=age_df, how='left', left_on='Country_Region', right_on='Location')\n",
    "#X_train.drop('Location', axis=1, inplace=True)\n",
    "#X_test.drop('Location', axis=1, inplace=True)\n",
    "#https://www.kaggle.com/danevans/world-bank-wdi-212-health-systems\n",
    "healthsys_df = pd.read_csv(\"../input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv\")\n",
    "healthsys_df.columns=[c.replace('-','_') for c in healthsys_df.columns]\n",
    "healthsys_df.drop('World_Bank_Name',axis=1,inplace=True)\n",
    "nan_country=healthsys_df[['Country_Region', 'Province_State']].isna().all(axis=1)\n",
    "healthsys_df=healthsys_df.loc[nan_country==False,:].reset_index(drop=True)\n",
    "#healthsys_df['Province_State']=healthsys_df['Province_State'].fillna('Unknown')\n",
    "healthsys_df[\"geography\"] = healthsys_df['Country_Region'].astype(str) + \": \" + healthsys_df['Province_State'].astype(str)\n",
    "healthsys_df.loc[healthsys_df['Province_State'].isna(), \"geography\"] = healthsys_df[healthsys_df['Province_State'].isna()]['Country_Region']\n",
    "healthsys_df.drop(['Country_Region','Province_State'], axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=healthsys_df, how='left', left_on=['Country_Region','Province_State'], right_on=['Country_Region', 'Province_State'])\n",
    "#X_test = pd.merge(left=X_test, right=healthsys_df, how='left', left_on=['Country_Region','Province_State'], right_on=['Country_Region', 'Province_State'])\n",
    "#https://www.kaggle.com/tanuprabhu/population-by-country-2020\n",
    "pop_df = pd.read_csv(\"../input/population-by-country-2020/population_by_country_2020.csv\")\n",
    "pop_df.columns=[c.replace('.',' ').split(' ')[0]+'_pop2020' for c in pop_df.columns]\n",
    "percent_col=['Yearly_pop2020','Urban_pop2020', 'World_pop2020']\n",
    "def depercent(x):\n",
    "    if x=='N.A.':\n",
    "        return np.nan \n",
    "    else:\n",
    "        return float(x.replace('%',''))\n",
    "for c in percent_col:\n",
    "    pop_df[c]=pop_df[c].apply(lambda x: depercent(x))\n",
    "pop_df=pop_df.replace('N.A.',np.nan)\n",
    "pop_df[['Population_pop2020', 'Yearly_pop2020',\n",
    "       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n",
    "       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']]=pop_df[['Population_pop2020', 'Yearly_pop2020',\n",
    "       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n",
    "       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']].astype(float)\n",
    "pop_df[\"geography\"] = pop_df.Country_pop2020.astype(str)\n",
    "pop_df.drop(\"Country_pop2020\", axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\n",
    "#X_test = pd.merge(left=X_test, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\n",
    "#X_train.drop('Country_pop2020', axis=1, inplace=True)\n",
    "#X_test.drop('Country_pop2020', axis=1, inplace=True)\n",
    "#https://www.kaggle.com/koryto/countryinfo\n",
    "compre_df = pd.read_csv(\"../input/countryinfo/covid19countryinfo.csv\")\n",
    "#compre_df['region']=compre_df['region'].fillna('Unknown')\n",
    "keepcol=['region', 'country', 'tests',\n",
    "       'testpop', 'density', 'medianage', 'urbanpop', 'quarantine', 'schools',\n",
    "       'publicplace', 'gatheringlimit', 'gathering', 'nonessential',\n",
    "       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n",
    "       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung', 'gdp2019',\n",
    "       'healthexp', 'healthperpop', 'fertility', 'firstcase']\n",
    "def tempfun(x):\n",
    "    if x is np.nan:\n",
    "        return x\n",
    "    else:\n",
    "        return float(x.replace(',',''))\n",
    "for c in ['gdp2019','healthexp']:\n",
    "    compre_df[c]=compre_df[c].apply(lambda x: tempfun(x) )\n",
    "    todate_col=['quarantine', 'schools','publicplace', 'gathering', 'nonessential','firstcase']\n",
    "for c in todate_col:\n",
    "    compre_df[c]= (pd.to_datetime(date.today())-pd.to_datetime(compre_df[c])).dt.days.astype(float)\n",
    "compre_df=compre_df[keepcol]\n",
    "compre_df[\"geography\"] = compre_df['country'].astype(str) + \": \" + compre_df['region'].astype(str)\n",
    "compre_df.loc[compre_df['region'].isna(), \"geography\"] = compre_df[compre_df['region'].isna()]['country']\n",
    "compre_df.drop(['country','region'], axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\n",
    "#X_test = pd.merge(left=X_test, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\n",
    "#X_train.drop(['country','region'], axis=1, inplace=True)\n",
    "#X_test.drop(['country','region'], axis=1, inplace=True)\n",
    "#https://www.kaggle.com/imdevskp/sars-outbreak-2003-complete-dataset\n",
    "sars_df = pd.read_csv(\"../input/sars-outbreak-2003-complete-dataset/sars_2003_complete_dataset_clean.csv\")\n",
    "def getProvince(x):\n",
    "    x_seg=x.split(',')\n",
    "    if len(x_seg)==2:\n",
    "        if 'SAR' in x_seg[0]:\n",
    "            return x_seg[0][:-4]\n",
    "        else:\n",
    "            return x_seg[0]\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "sars_df['Province']=sars_df['Country'].apply(lambda x: getProvince(x))\n",
    "sars_df['Country']=sars_df['Country'].apply(lambda x: x.split(',')[-1])\n",
    "sars_df['Country']=sars_df['Country'].replace('Viet Nam','Vietnam')\n",
    "def getSlope(ses,segs):\n",
    "    segsize=np.floor(len(ses)/segs)\n",
    "    slope=[]\n",
    "    for i in range(segs):\n",
    "        if i==segs-1:\n",
    "            slope.append((ses[-1]-ses[int(i*segsize)])/(len(ses)-1-i*segsize))\n",
    "        else:\n",
    "            slope.append((ses[int((i+1)*segsize-1)]-ses[int(i*segsize)])/(segsize-1))\n",
    "    return slope   \n",
    "def aggSARS(df):\n",
    "    case=df['Cumulative number of case(s)']\n",
    "    death=df['Number of deaths'].cumsum()\n",
    "    recover=df['Number recovered'].cumsum()\n",
    "    Sars_dict={}\n",
    "    Sars_dict['CaseMax']=case.max()\n",
    "    Sars_dict['DeathMax']=death.max()\n",
    "    Sars_dict['RecoverMax']=recover.max()\n",
    "    segs=df['Date'].apply(lambda x: x.split('-')[1]).nunique()\n",
    "    for i,s in enumerate(getSlope(case.values,segs)):\n",
    "        Sars_dict['Case_'+str(i)]=s\n",
    "    for i,s in enumerate(getSlope(death.values,segs)):\n",
    "        Sars_dict['Death_'+str(i)]=s\n",
    "    for i,s in enumerate(getSlope(recover.values,segs)):\n",
    "        Sars_dict['Recover_'+str(i)]=s\n",
    "    return pd.DataFrame(Sars_dict,index=[0])\n",
    "sars_df_grouped=sars_df.groupby(['Country','Province']).apply(aggSARS).reset_index().drop('level_2',axis=1)\n",
    "sars_df_grouped[\"geography\"] = sars_df_grouped['Country'].astype(str) + \": \" + sars_df_grouped['Province'].astype(str)\n",
    "sars_df_grouped.loc[sars_df_grouped['Province'].isna(), \"geography\"] = sars_df_grouped[sars_df_grouped['Province'].isna()]['Country']\n",
    "sars_df_grouped.drop(['Country','Province'], axis=1, inplace=True)\n",
    "#X_train = pd.merge(left=X_train, right=sars_df_grouped, how='left', left_on=['Country_Region','Province_State'], right_on=['Country','Province'])\n",
    "#X_test = pd.merge(left=X_test, right=sars_df_grouped, how='left', left_on=['Country_Region','Province_State'], right_on=['Country','Province'])\n",
    "#X_train.drop(['Country','Province'], axis=1, inplace=True)\n",
    "#X_test.drop(['Country','Province'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#f_cat=['Country_Region','Province_State','Regional_indicator']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I borrowed the frame in [Vopani's work in week 2](https://www.kaggle.com/rohanrao/covid-19-w2-lgb-mad) to save some time. He did a excellent work there. I didn't use his model on moving average though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## defining constants\n",
    "VAL_DAYS = 7\n",
    "MAD_FACTOR = 0.5\n",
    "DAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n",
    "\n",
    "SEED = 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading data\n",
    "train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/test.csv')\n",
    "#https://www.kaggle.com/rohanrao/covid19-forecasting-metadata\n",
    "region_metadata = pd.read_csv('/kaggle/input/covid19-forecasting-metadata/region_metadata.csv')\n",
    "region_date_metadata = pd.read_csv('/kaggle/input/covid19-forecasting-metadata/region_date_metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing data\n",
    "train = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\n",
    "test = test[~test.Date.isin(train.Date.unique())]\n",
    "\n",
    "df_panel = pd.concat([train, test], sort = False)\n",
    "\n",
    "# combining state and country into 'geography'\n",
    "df_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\n",
    "df_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n",
    "\n",
    "# fixing data issues with cummax\n",
    "df_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\n",
    "df_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n",
    "\n",
    "# merging external metadata\n",
    "df_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"], how = \"left\")\n",
    "df_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n",
    "\n",
    "# label encoding continent\n",
    "df_panel.continent = MyLabelEncodeSingle(df_panel.continent)\n",
    "df_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n",
    "\n",
    "df_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train date range: 2020-01-22 00:00:00  -  2020-04-06 00:00:00\n",
      "Test date range: 2020-03-26 00:00:00  -  2020-05-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "## feature engineering\n",
    "min_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\n",
    "max_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n",
    "\n",
    "min_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\n",
    "max_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n",
    "\n",
    "n_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n",
    "\n",
    "print(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\n",
    "print(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n",
    "\n",
    "# creating lag features\n",
    "for lag in range(1, 41):\n",
    "    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n",
    "    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n",
    "    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n",
    "\n",
    "for case in DAYS_SINCE_CASES:\n",
    "    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to merge my added data to main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all extra features calculated above\n",
    "def extrafeatures(df):\n",
    "    #print('before: {}'.format(len(df)))\n",
    "    has_col=df.columns.tolist()\n",
    "    df['UpToNow']=(pd.to_datetime(date.today())-pd.to_datetime(df['Date'])).dt.days.astype(float)\n",
    "   # print('after UpToNow: {}'.format(len(df)))\n",
    "    df = df.merge(world_happiness_grouped, how='left', on='geography')\n",
    "    #print('after world_happiness: {}'.format(len(df)))\n",
    "    df = df.merge(malaria_world_health, how='left', on='geography')\n",
    "   # print('after malaria: {}'.format(len(df)))\n",
    "    df = df.merge(human_development, how='left', on='geography')\n",
    "   # print('after human: {}'.format(len(df)))\n",
    "    df = df.merge(night_ranger, how='left', on='geography')\n",
    "    #print('after night: {}'.format(len(df)))\n",
    "    df['month']=df['Date'].dt.month\n",
    "    df = df.merge(weather_df, how='left', on=['geography','month']).drop('month',axis=1)#month\n",
    "    #print('after weather: {}'.format(len(df)))\n",
    "    df = df.merge(happiness_df, how='left', on='geography')\n",
    "    #print('after happiness: {}'.format(len(df)))\n",
    "    df = df.merge(age_df, how='left', on='geography')\n",
    "    #print('after age: {}'.format(len(df)))\n",
    "    df = df.merge(healthsys_df, how='left', on='geography')\n",
    "    #print('after healthsys: {}'.format(len(df)))\n",
    "    df = df.merge(pop_df, how='left', on='geography')\n",
    "    #print('after pop: {}'.format(len(df)))\n",
    "    df = df.merge(compre_df, how='left', on='geography')\n",
    "    #print('after compre: {}'.format(len(df)))\n",
    "    df = df.merge(sars_df_grouped, how='left', on='geography')\n",
    "    #print('after sars: {}'.format(len(df)))\n",
    "    extra_col=[c for c in df.columns.tolist() if c not in has_col]\n",
    "    return df,extra_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for preparing features\n",
    "def prepare_features(df, gap):\n",
    "    \n",
    "    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) / df[f\"lag_{gap}_cc\"]\n",
    "    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] / df.population\n",
    "    \n",
    "    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n",
    "    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n",
    "    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n",
    "    \n",
    "    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n",
    "    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n",
    "    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n",
    "    \n",
    "    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) / 3\n",
    "    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) / 3\n",
    "\n",
    "    df[\"diff_change_1_cc\"] = df.diff_1_cc / df.diff_2_cc\n",
    "    df[\"diff_change_2_cc\"] = df.diff_2_cc / df.diff_3_cc\n",
    "    \n",
    "    df[\"diff_change_1_ft\"] = df.diff_1_ft / df.diff_2_ft\n",
    "    df[\"diff_change_2_ft\"] = df.diff_2_ft / df.diff_3_ft\n",
    "\n",
    "    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) / 2\n",
    "    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) / 2\n",
    "    \n",
    "    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 1}_cc\"]\n",
    "    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] / df[f\"lag_{gap + 2}_cc\"]\n",
    "    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n",
    "\n",
    "    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 1}_ft\"]\n",
    "    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] / df[f\"lag_{gap + 2}_ft\"]\n",
    "    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n",
    "\n",
    "    df[\"change_123_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n",
    "    df[\"change_123_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n",
    "    \n",
    "    for case in DAYS_SINCE_CASES:\n",
    "        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n",
    "        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n",
    "\n",
    "    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n",
    "    df[\"density\"] = df.population / df.area\n",
    "    \n",
    "    # target variable is log of change from last known value\n",
    "    df[\"target_cc\"] = np.log1p(df.ConfirmedCases) - np.log1p(df[f\"lag_{gap}_cc\"])\n",
    "    df[\"target_ft\"] = np.log1p(df.Fatalities) - np.log1p(df[f\"lag_{gap}_ft\"])\n",
    "    \n",
    "    df,extra_col=extrafeatures(df.copy())\n",
    "    \n",
    "    features = [\n",
    "        f\"lag_{gap}_cc\",\n",
    "        f\"lag_{gap}_ft\",\n",
    "        f\"lag_{gap}_rc\",\n",
    "        \"perc_1_ac\",\n",
    "        \"perc_1_cc\",\n",
    "        \"diff_1_cc\",\n",
    "        \"diff_2_cc\",\n",
    "        \"diff_3_cc\",\n",
    "        \"diff_1_ft\",\n",
    "        \"diff_2_ft\",\n",
    "        \"diff_3_ft\",\n",
    "        \"diff_123_cc\",\n",
    "        \"diff_123_ft\",\n",
    "        \"diff_change_1_cc\",\n",
    "        \"diff_change_2_cc\",\n",
    "        \"diff_change_1_ft\",\n",
    "        \"diff_change_2_ft\",\n",
    "        \"diff_change_12_cc\",\n",
    "        \"diff_change_12_ft\",\n",
    "        \"change_1_cc\",\n",
    "        \"change_2_cc\",\n",
    "        \"change_3_cc\",\n",
    "        \"change_1_ft\",\n",
    "        \"change_2_ft\",\n",
    "        \"change_3_ft\",\n",
    "        \"change_123_cc\",\n",
    "        \"change_123_ft\",\n",
    "        \"days_since_1_case\",\n",
    "        \"days_since_10_case\",\n",
    "        \"days_since_50_case\",\n",
    "        \"days_since_100_case\",\n",
    "        \"days_since_500_case\",\n",
    "        \"days_since_1000_case\",\n",
    "        \"days_since_5000_case\",\n",
    "        \"days_since_10000_case\",\n",
    "        \"country_flag\",\n",
    "        #\"lat\",\n",
    "        #\"lon\",\n",
    "        \"continent\",\n",
    "        #\"population\",\n",
    "        \"area\",\n",
    "        \"density\",\n",
    "        \"target_cc\",\n",
    "        \"target_ft\"\n",
    "    ]+extra_col\n",
    "    \n",
    "    return df[features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for building and predicting using LGBM model\n",
    "def build_predict_lgbm(df_train, df_test, gap):\n",
    "    LGB_PARAMS_C = {\"objective\": \"regression\",\n",
    "              \"num_leaves\": 7,\n",
    "              \"learning_rate\": 0.1,\n",
    "              \"bagging_fraction\": 0.91,\n",
    "              \"feature_fraction\": 0.81,\n",
    "              \"reg_alpha\": 0.05,\n",
    "              \"reg_lambda\": 0.13,\n",
    "              \"metric\": \"rmse\",\n",
    "              \"seed\": SEED\n",
    "             }\n",
    "    \n",
    "    LGB_PARAMS_F = {\"objective\": \"regression\",\n",
    "              \"num_leaves\": 9,\n",
    "              \"learning_rate\": 0.1,#0.013,\n",
    "              \"bagging_fraction\": 0.91,\n",
    "              \"feature_fraction\": 0.25,#0.81,\n",
    "              #\"min_data_in_leaf\" : 50,\n",
    "              \"reg_alpha\": 0.13,\n",
    "              \"reg_lambda\": 0.13,\n",
    "              \"metric\": \"rmse\",\n",
    "              \"seed\": SEED\n",
    "             }\n",
    "    \n",
    "    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n",
    "    \n",
    "    target_cc = df_train.target_cc\n",
    "    target_ft = df_train.target_ft\n",
    "    \n",
    "    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n",
    "    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n",
    "    \n",
    "    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n",
    "    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n",
    "    \n",
    "    categorical_features = ['continent','Regional_indicator']\n",
    "    \n",
    "    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n",
    "    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n",
    "\n",
    "    model_cc = lgb.train(LGB_PARAMS_C, train_set = dtrain_cc, num_boost_round = 200)\n",
    "    model_ft = lgb.train(LGB_PARAMS_F, train_set = dtrain_ft, num_boost_round = 1000)\n",
    "    \n",
    "    # inverse transform from log of change from last known value\n",
    "    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_cc))\n",
    "    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 1000) + np.log1p(test_lag_ft))\n",
    "    \n",
    "    return y_pred_cc, y_pred_ft, model_cc, model_ft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for predicting moving average decay model\n",
    "def predict_mad(df_test, gap, val = False):\n",
    "    \n",
    "    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) / 3\n",
    "    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) / 3\n",
    "\n",
    "    if val:\n",
    "        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / VAL_DAYS\n",
    "        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / VAL_DAYS\n",
    "    else:\n",
    "        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / n_dates_test\n",
    "        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / n_dates_test\n",
    "\n",
    "    return y_pred_cc, y_pred_ft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 2020-03-26T00:00:00.000000000\n",
      "Processing date: 2020-03-27T00:00:00.000000000\n",
      "Processing date: 2020-03-28T00:00:00.000000000\n",
      "Processing date: 2020-03-29T00:00:00.000000000\n",
      "Processing date: 2020-03-30T00:00:00.000000000\n",
      "Processing date: 2020-03-31T00:00:00.000000000\n",
      "Processing date: 2020-04-01T00:00:00.000000000\n",
      "Processing date: 2020-04-02T00:00:00.000000000\n",
      "Processing date: 2020-04-03T00:00:00.000000000\n",
      "Processing date: 2020-04-04T00:00:00.000000000\n",
      "Processing date: 2020-04-05T00:00:00.000000000\n",
      "Processing date: 2020-04-06T00:00:00.000000000\n",
      "Processing date: 2020-04-07T00:00:00.000000000\n",
      "Processing date: 2020-04-08T00:00:00.000000000\n",
      "Processing date: 2020-04-09T00:00:00.000000000\n",
      "Processing date: 2020-04-10T00:00:00.000000000\n",
      "Processing date: 2020-04-11T00:00:00.000000000\n",
      "Processing date: 2020-04-12T00:00:00.000000000\n",
      "Processing date: 2020-04-13T00:00:00.000000000\n",
      "Processing date: 2020-04-14T00:00:00.000000000\n",
      "Processing date: 2020-04-15T00:00:00.000000000\n",
      "Processing date: 2020-04-16T00:00:00.000000000\n",
      "Processing date: 2020-04-17T00:00:00.000000000\n",
      "Processing date: 2020-04-18T00:00:00.000000000\n",
      "Processing date: 2020-04-19T00:00:00.000000000\n",
      "Processing date: 2020-04-20T00:00:00.000000000\n",
      "Processing date: 2020-04-21T00:00:00.000000000\n",
      "Processing date: 2020-04-22T00:00:00.000000000\n",
      "Processing date: 2020-04-23T00:00:00.000000000\n",
      "Processing date: 2020-04-24T00:00:00.000000000\n",
      "Processing date: 2020-04-25T00:00:00.000000000\n",
      "Processing date: 2020-04-26T00:00:00.000000000\n",
      "Processing date: 2020-04-27T00:00:00.000000000\n",
      "Processing date: 2020-04-28T00:00:00.000000000\n",
      "Processing date: 2020-04-29T00:00:00.000000000\n",
      "Processing date: 2020-04-30T00:00:00.000000000\n",
      "Processing date: 2020-05-01T00:00:00.000000000\n",
      "Processing date: 2020-05-02T00:00:00.000000000\n",
      "Processing date: 2020-05-03T00:00:00.000000000\n",
      "Processing date: 2020-05-04T00:00:00.000000000\n",
      "Processing date: 2020-05-05T00:00:00.000000000\n",
      "Processing date: 2020-05-06T00:00:00.000000000\n",
      "Processing date: 2020-05-07T00:00:00.000000000\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "## building lag x-days models\n",
    "df_train = df_panel[~df_panel.Id.isna()]\n",
    "df_test_full = df_panel[~df_panel.ForecastId.isna()]\n",
    "\n",
    "df_preds_val = []\n",
    "df_preds_test = []\n",
    "\n",
    "for pdate in df_test_full.Date.unique():\n",
    "    \n",
    "    print(\"Processing date:\", pdate)\n",
    "    \n",
    "    # ignore date already present in train data\n",
    "    if pdate in df_train.Date.values:\n",
    "        df_pred_test = df_test_full.loc[df_test_full.Date == pdate, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n",
    "        \n",
    "        # multiplying predictions by 41 to not look cool on public LB\n",
    "        df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n",
    "        df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n",
    "    else:\n",
    "        df_test = df_test_full[df_test_full.Date == pdate]\n",
    "        \n",
    "        gap = (pd.Timestamp(pdate) - max_date_train).days\n",
    "        \n",
    "        if gap <= VAL_DAYS:\n",
    "            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n",
    "\n",
    "            df_build = df_train[df_train.Date < val_date]\n",
    "            df_val = df_train[df_train.Date == val_date]\n",
    "            \n",
    "            X_build = prepare_features(df_build, gap)\n",
    "            X_val = prepare_features(df_val, gap)\n",
    "            \n",
    "            #print('len of df_val{}, len of X_val{}'.format(len(df_val),len(X_val)) )\n",
    "            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n",
    "            #y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n",
    "            #print('{}_{}_{}'.format(len(df_val.Id.values),len(y_val_cc_lgb),len(y_val_ft_lgb)))\n",
    "            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n",
    "                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n",
    "                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n",
    "                                       # \"ConfirmedCases_val_mad\": y_val_cc_mad,\n",
    "                                      #  \"Fatalities_val_mad\": y_val_ft_mad,\n",
    "                                       })\n",
    "\n",
    "            df_preds_val.append(df_pred_val)\n",
    "\n",
    "        X_train = prepare_features(df_train, gap)\n",
    "        X_test = prepare_features(df_test, gap)\n",
    "\n",
    "        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n",
    "       # y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n",
    "        \n",
    "        if gap == 1:\n",
    "            model_1_cc = model_cc\n",
    "            model_1_ft = model_ft\n",
    "            features_1 = X_train.columns.values\n",
    "        elif gap == 14:\n",
    "            model_14_cc = model_cc\n",
    "            model_14_ft = model_ft\n",
    "            features_14 = X_train.columns.values\n",
    "        elif gap == 28:\n",
    "            model_28_cc = model_cc\n",
    "            model_28_ft = model_ft\n",
    "            features_28 = X_train.columns.values\n",
    "        \n",
    "        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n",
    "                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n",
    "                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n",
    "                                   #  \"ConfirmedCases_test_mad\": y_test_cc_mad,\n",
    "                                  #   \"Fatalities_test_mad\": y_test_ft_mad,\n",
    "                                    })\n",
    "    \n",
    "    df_preds_test.append(df_pred_test)\n",
    "print(len(X_val.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB CC RMSLE Val of 7 days for CC: 0.2\n",
      "LGB FT RMSLE Val of 7 days for FT: 0.24\n",
      "LGB Overall RMSLE Val of 7 days: 0.22\n"
     ]
    }
   ],
   "source": [
    "## validation score\n",
    "df_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\n",
    "df_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n",
    "\n",
    "rmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\n",
    "rmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n",
    "\n",
    "#rmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\n",
    "#rmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n",
    "\n",
    "print(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\n",
    "print(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\n",
    "print(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) / 2, 2))\n",
    "#print(\"\\n\")\n",
    "#print(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\n",
    "#print(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\n",
    "#print(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) / 2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\": \"JpP8FXbgAZLkfur7LiK3j9AGBhHNIvF742meBJrjO2ShJDhCG2I1uVvW+0DUtrmc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\": \"xZlADit0Q04ISQEdKg2k3L4W9AwQBAuDs9nJL9fM/WwzL1tEU9VPNezOFX0nLEAz\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\": \"4BuPRZkdMKSnj3zoxiNrQ86XgNw0rYmBOxe7nshquXwwcauupgBF2DHLVG1WuZlV\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\": \"Dv1SQ87hmDqK6S5OhBf0bCuwAEvL5QYL0PuR/F1SPVhCS/r/abjkbpKDYL2zeM19\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.0.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.0.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"7c7ff335-a264-4d4c-9297-fdabbbc80e24\" data-root-id=\"1113\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"9795943d-b757-4848-a139-549f3d0ce614\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1002\"},{\"id\":\"1039\"},{\"id\":\"1076\"}]},\"id\":\"1113\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"Feature Importance of LGB Model 1\"},\"id\":\"1003\",\"type\":\"Title\"},{\"attributes\":{\"source\":{\"id\":\"1071\"}},\"id\":\"1075\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1},\"x\":{\"field\":\"x\"}},\"id\":\"1073\",\"type\":\"VBar\"},{\"attributes\":{\"data_source\":{\"id\":\"1071\"},\"glyph\":{\"id\":\"1072\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1073\"},\"selection_glyph\":null,\"view\":{\"id\":\"1075\"}},\"id\":\"1074\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data\":{\"top\":{\"__ndarray__\":\"zAAAAD0AAAA8AAAANQAAACsAAAAqAAAAKQAAACYAAAAkAAAAIgAAAB4AAAAcAAAAHAAAABQAAAAUAAAAEgAAABAAAAANAAAADQAAAAwAAAALAAAACwAAAAsAAAALAAAACwAAAA==\",\"dtype\":\"int32\",\"shape\":[25]},\"x\":[\"UpToNow\",\"area\",\"density_x\",\"perc_1_cc\",\"pop\",\"firstcase\",\"humidity\",\"long\",\"temperature\",\"lag_28_cc\",\"smokers_x\",\"quarantine\",\"lat_x\",\"schools\",\"health_exp_pc\",\"medianage_y\",\"gdp2019\",\"perc_1_ac\",\"lag_28_rc\",\"days_since_5000_case\",\"urbanpop_y\",\"Females_2018\",\"days_since_10000_case\",\"Regional_indicator\",\"diff_123_cc\"]},\"selected\":{\"id\":\"1136\"},\"selection_policy\":{\"id\":\"1137\"}},\"id\":\"1108\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"below\":[{\"id\":\"1087\"}],\"center\":[{\"id\":\"1089\"},{\"id\":\"1093\"}],\"left\":[{\"id\":\"1090\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1111\"}],\"title\":{\"id\":\"1077\"},\"toolbar\":{\"id\":\"1101\"},\"x_range\":{\"id\":\"1079\"},\"x_scale\":{\"id\":\"1083\"},\"y_range\":{\"id\":\"1081\"},\"y_scale\":{\"id\":\"1085\"}},\"id\":\"1076\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"text\":\"Feature Importance of LGB Model 28\"},\"id\":\"1077\",\"type\":\"Title\"},{\"attributes\":{\"factors\":[\"UpToNow\",\"area\",\"density_x\",\"perc_1_cc\",\"pop\",\"firstcase\",\"humidity\",\"long\",\"temperature\",\"lag_28_cc\",\"smokers_x\",\"quarantine\",\"lat_x\",\"schools\",\"health_exp_pc\",\"medianage_y\",\"gdp2019\",\"perc_1_ac\",\"lag_28_rc\",\"days_since_5000_case\",\"urbanpop_y\",\"Females_2018\",\"days_since_10000_case\",\"Regional_indicator\",\"diff_123_cc\"]},\"id\":\"1079\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"1085\",\"type\":\"LinearScale\"},{\"attributes\":{\"formatter\":{\"id\":\"1127\"},\"major_label_orientation\":1.3,\"ticker\":{\"id\":\"1088\"}},\"id\":\"1087\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1087\"},\"ticker\":null},\"id\":\"1089\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"1125\"},\"ticker\":{\"id\":\"1091\"}},\"id\":\"1090\",\"type\":\"LinearAxis\"},{\"attributes\":{\"factors\":[\"lag_1_cc\",\"days_since_10_case\",\"days_since_50_case\",\"days_since_100_case\",\"change_123_cc\",\"UpToNow\",\"days_since_500_case\",\"days_since_1_case\",\"change_1_cc\",\"diff_123_cc\",\"days_since_1000_case\",\"perc_1_cc\",\"change_3_cc\",\"perc_1_ac\",\"change_2_cc\",\"days_since_5000_case\",\"days_since_10000_case\",\"diff_change_1_cc\",\"diff_3_cc\",\"long\",\"diff_1_cc\",\"lag_1_rc\",\"diff_change_12_cc\",\"diff_change_2_cc\",\"lat_x\"]},\"id\":\"1005\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1091\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1094\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"1095\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"overlay\":{\"id\":\"1100\"}},\"id\":\"1096\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1119\"},\"major_label_orientation\":1.3,\"ticker\":{\"id\":\"1014\"}},\"id\":\"1013\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1097\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"1098\",\"type\":\"ResetTool\"},{\"attributes\":{\"axis\":{\"id\":\"1013\"},\"ticker\":null},\"id\":\"1015\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1099\",\"type\":\"HelpTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1117\"},\"ticker\":{\"id\":\"1017\"}},\"id\":\"1016\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1137\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1094\"},{\"id\":\"1095\"},{\"id\":\"1096\"},{\"id\":\"1097\"},{\"id\":\"1098\"},{\"id\":\"1099\"}]},\"id\":\"1101\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1017\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1016\"},\"dimension\":1,\"ticker\":null},\"id\":\"1019\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1134\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1136\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"top\":{\"__ndarray__\":\"kwAAAGoAAABfAAAAOwAAADUAAAA1AAAAMgAAAC4AAAApAAAAJgAAACMAAAAjAAAAHgAAABgAAAAWAAAAFQAAABAAAAAPAAAADQAAAAwAAAAMAAAACwAAAAsAAAAKAAAACQAAAA==\",\"dtype\":\"int32\",\"shape\":[25]},\"x\":[\"lag_1_cc\",\"days_since_10_case\",\"days_since_50_case\",\"days_since_100_case\",\"change_123_cc\",\"UpToNow\",\"days_since_500_case\",\"days_since_1_case\",\"change_1_cc\",\"diff_123_cc\",\"days_since_1000_case\",\"perc_1_cc\",\"change_3_cc\",\"perc_1_ac\",\"change_2_cc\",\"days_since_5000_case\",\"days_since_10000_case\",\"diff_change_1_cc\",\"diff_3_cc\",\"long\",\"diff_1_cc\",\"lag_1_rc\",\"diff_change_12_cc\",\"diff_change_2_cc\",\"lat_x\"]},\"selected\":{\"id\":\"1130\"},\"selection_policy\":{\"id\":\"1131\"}},\"id\":\"1034\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1133\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1020\",\"type\":\"PanTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1100\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1026\"}},\"id\":\"1022\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1},\"x\":{\"field\":\"x\"}},\"id\":\"1110\",\"type\":\"VBar\"},{\"attributes\":{\"data_source\":{\"id\":\"1108\"},\"glyph\":{\"id\":\"1109\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1110\"},\"selection_glyph\":null,\"view\":{\"id\":\"1112\"}},\"id\":\"1111\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1},\"x\":{\"field\":\"x\"}},\"id\":\"1109\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"HelpTool\"},{\"attributes\":{\"source\":{\"id\":\"1108\"}},\"id\":\"1112\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1},\"x\":{\"field\":\"x\"}},\"id\":\"1035\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1020\"},{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"}]},\"id\":\"1027\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1117\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1119\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1121\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1},\"x\":{\"field\":\"x\"}},\"id\":\"1036\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1123\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1125\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1026\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1127\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1034\"}},\"id\":\"1038\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1130\",\"type\":\"Selection\"},{\"attributes\":{\"below\":[{\"id\":\"1050\"}],\"center\":[{\"id\":\"1052\"},{\"id\":\"1056\"}],\"left\":[{\"id\":\"1053\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1074\"}],\"title\":{\"id\":\"1040\"},\"toolbar\":{\"id\":\"1064\"},\"x_range\":{\"id\":\"1042\"},\"x_scale\":{\"id\":\"1046\"},\"y_range\":{\"id\":\"1044\"},\"y_scale\":{\"id\":\"1048\"}},\"id\":\"1039\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1131\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"1034\"},\"glyph\":{\"id\":\"1035\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1036\"},\"selection_glyph\":null,\"view\":{\"id\":\"1038\"}},\"id\":\"1037\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"text\":\"Feature Importance of LGB Model 14\"},\"id\":\"1040\",\"type\":\"Title\"},{\"attributes\":{\"factors\":[\"UpToNow\",\"perc_1_cc\",\"density_x\",\"lag_14_cc\",\"firstcase\",\"area\",\"long\",\"days_since_100_case\",\"pop\",\"days_since_5000_case\",\"days_since_500_case\",\"temperature\",\"days_since_1_case\",\"quarantine\",\"change_123_cc\",\"days_since_10000_case\",\"days_since_1000_case\",\"humidity\",\"change_1_cc\",\"gdp2019\",\"perc_1_ac\",\"schools\",\"health_exp_pc\",\"density_y\",\"smokers_x\"]},\"id\":\"1042\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1044\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"LinearScale\"},{\"attributes\":{\"formatter\":{\"id\":\"1123\"},\"major_label_orientation\":1.3,\"ticker\":{\"id\":\"1051\"}},\"id\":\"1050\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1050\"},\"ticker\":null},\"id\":\"1052\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"1121\"},\"ticker\":{\"id\":\"1054\"}},\"id\":\"1053\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1054\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1053\"},\"dimension\":1,\"ticker\":null},\"id\":\"1056\",\"type\":\"Grid\"},{\"attributes\":{\"data\":{\"top\":{\"__ndarray__\":\"tgAAAEYAAAA1AAAANAAAACsAAAAkAAAAHQAAABwAAAAcAAAAGgAAABoAAAAZAAAAGAAAABcAAAAVAAAAEwAAABIAAAARAAAAEQAAABAAAAAQAAAAEAAAAA8AAAAOAAAADgAAAA==\",\"dtype\":\"int32\",\"shape\":[25]},\"x\":[\"UpToNow\",\"perc_1_cc\",\"density_x\",\"lag_14_cc\",\"firstcase\",\"area\",\"long\",\"days_since_100_case\",\"pop\",\"days_since_5000_case\",\"days_since_500_case\",\"temperature\",\"days_since_1_case\",\"quarantine\",\"change_123_cc\",\"days_since_10000_case\",\"days_since_1000_case\",\"humidity\",\"change_1_cc\",\"gdp2019\",\"perc_1_ac\",\"schools\",\"health_exp_pc\",\"density_y\",\"smokers_x\"]},\"selected\":{\"id\":\"1133\"},\"selection_policy\":{\"id\":\"1134\"}},\"id\":\"1071\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"below\":[{\"id\":\"1013\"}],\"center\":[{\"id\":\"1015\"},{\"id\":\"1019\"}],\"left\":[{\"id\":\"1016\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1037\"}],\"title\":{\"id\":\"1003\"},\"toolbar\":{\"id\":\"1027\"},\"x_range\":{\"id\":\"1005\"},\"x_scale\":{\"id\":\"1009\"},\"y_range\":{\"id\":\"1007\"},\"y_scale\":{\"id\":\"1011\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1057\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1058\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1063\"}},\"id\":\"1059\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1060\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1061\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1062\",\"type\":\"HelpTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":1},\"x\":{\"field\":\"x\"}},\"id\":\"1072\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1057\"},{\"id\":\"1058\"},{\"id\":\"1059\"},{\"id\":\"1060\"},{\"id\":\"1061\"},{\"id\":\"1062\"}]},\"id\":\"1064\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis\":{\"id\":\"1090\"},\"dimension\":1,\"ticker\":null},\"id\":\"1093\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1063\",\"type\":\"BoxAnnotation\"}],\"root_ids\":[\"1113\"]},\"title\":\"Bokeh Application\",\"version\":\"2.0.1\"}};\n",
       "  var render_items = [{\"docid\":\"9795943d-b757-4848-a139-549f3d0ce614\",\"root_ids\":[\"1113\"],\"roots\":{\"1113\":\"7c7ff335-a264-4d4c-9297-fdabbbc80e24\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1113"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## feature importance\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column\n",
    "from bokeh.palettes import Spectral3\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "df_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\n",
    "df_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\n",
    "df_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n",
    "\n",
    "df_fimp_1_cc.sort_values(\"importance\", ascending = False, inplace = True)\n",
    "df_fimp_14_cc.sort_values(\"importance\", ascending = False, inplace = True)\n",
    "df_fimp_28_cc.sort_values(\"importance\", ascending = False, inplace = True)\n",
    "\n",
    "v1 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_1_cc.feature[:25], title = \"Feature Importance of LGB Model 1\")\n",
    "v1.vbar(x = df_fimp_1_cc.feature[:25], top = df_fimp_1_cc.importance[:25], width = 1)\n",
    "v1.xaxis.major_label_orientation = 1.3\n",
    "\n",
    "v14 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_14_cc.feature[:25], title = \"Feature Importance of LGB Model 14\")\n",
    "v14.vbar(x = df_fimp_14_cc.feature[:25], top = df_fimp_14_cc.importance[:25], width = 1)\n",
    "v14.xaxis.major_label_orientation = 1.3\n",
    "\n",
    "v28 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_28_cc.feature[:25], title = \"Feature Importance of LGB Model 28\")\n",
    "v28.vbar(x = df_fimp_28_cc.feature[:25], top = df_fimp_28_cc.importance[:25], width = 1)\n",
    "v28.xaxis.major_label_orientation = 1.3\n",
    "\n",
    "v = column(v1, v14, v28)\n",
    "\n",
    "show(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n",
    "                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\",\n",
    "                                                     \"Fatalities_test\", \"Fatalities_test_lgb\"]].reset_index()\n",
    "df_test[\"ConfirmedCases\"] = df_test.ConfirmedCases_test_lgb\n",
    "df_test[\"Fatalities\"] = df_test.Fatalities_test_lgb\n",
    "df_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\n",
    "df_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n",
    "\n",
    "df_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\n",
    "df_submission.ForecastId = df_submission.ForecastId.astype(int)\n",
    "df_submission.to_csv('submission.csv', index = False)"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
